# DS-AutoAdvisor ğŸš€

**Smart ML Pipeline: From Raw CSV to Production-Ready Models in 3 Steps + Advanced Plugins**

> Turn your messy data into clean, trained models with deployment-ready artifacts. Transparent, cacheable, and human-guided every step of the way. Enhanced with hyperparameter optimization and data drift monitoring.

## ğŸ¯ What This Does For You

**THE PROBLEM:** Traditional ML pipelines are slow, opaque, and repetitive. You waste hours on data profiling, mysterious cleaning steps, and packaging models for deployment.

**THE SOLUTION:** DS-AutoAdvisor gives you:
- âš¡ **One-time deep data discovery** (no more re-profiling the same data)
- ğŸ” **Transparent column-by-column cleaning** (you control every transformation)
- ğŸ§ª **Fast iterative testing** (test cleaning/training separately without full reruns)
- ğŸ“¦ **Production packaging** (deployment scripts, docs, APIs automatically generated)
- ğŸ“Š **Full traceability** (MLflow tracking + structured JSON outputs)
- ï¿½ **Smart HPO Integration** (automatic hyperparameter optimization with Optuna)
- ğŸ“Š **Data Drift Monitoring** (production vs training data monitoring with Evidently)

## ğŸ—ï¸ Architecture

```
ğŸ“Š Raw Data
    â†“
ğŸ” Step 1: Deep Discovery (profiles + generates cleaning template)
    â†“ 
âœï¸  HUMAN STEP: Edit cleaning_config_template.yaml 
    â†“
ğŸ§ª Step 2: Test Pipeline Stages (cleaning â†’ training â†’ evaluation)
    â†“
ğŸ“¦ Step 3: Package Best Models (deployment scripts + docs + APIs)
    â†“
ğŸ§© Optional: Advanced Plugins (HPO + Monitoring)
```

### ğŸ§© Plugin System
- **ğŸ¯ Optuna HPO**: Intelligent hyperparameter optimization with auto-generated parameter spaces
- **ğŸ“Š Evidently Monitor**: Data drift detection and performance monitoring with Slack integration

## ğŸ“‹ Table of Contents
1. [Quick Start (5 Minutes)](#quick-start)
2. [Plugin System](#plugin-system)
3. [Enhanced Data Quality System](#enhanced-data-quality-system)
4. [The 3-Step Workflow](#workflow)
5. [Important Manual Steps](#manual-steps)
6. [What You Get From Each Step](#outputs)
7. [Common Use Cases](#use-cases)
8. [Configuration Guide](#configuration)
9. [MLflow Tracking](#mlflow)
10. [Design Choices & Limitations](#limitations)
11. [Troubleshooting](#troubleshooting)
12. [Future Features](#roadmap)

---
## Quick Start

**ğŸ¯ Goal:** Get from CSV to trained models in 15 minutes

```bash
# Step 1: Analyze your data (one-time, 3-5 minutes)
python 01_data_discovery.py --data data/your_data.csv --target your_target_column

# ğŸ›‘ STOP: Edit the generated template 
# Open: pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml
# Customize column cleaning rules (imputation, outliers, encoding, etc.)

# Step 2: Test your pipeline (iterative, 2-10 minutes)
python 02_stage_testing.py --stage all

# Step 3: Package for production (1-3 minutes)  
python 03_full_pipeline.py --enable-mlflow

# Optional: View experiment tracking
./mlflow_ui.sh
```

**That's it!** Your models, deployment scripts, and documentation are ready.

---

## ğŸ§© Plugin System

**Enhance your pipeline with advanced ML capabilities:**

### ğŸ¯ Optuna HPO Plugin
Intelligent hyperparameter optimization that learns from your data:

```bash
# Check plugin status
python plugin_manager.py status

# Generate optimized HPO config for your dataset
python plugin_manager.py generate-hpo --data data/your_data.csv --target your_target --mode custom

# Generated config includes model recommendations and parameter spaces
```

**Features:**
- ğŸ“Š **Auto Analysis**: Analyzes dataset characteristics (size, features, target type)
- ğŸ¤– **Smart Recommendations**: Suggests optimal models for your data
- âš™ï¸ **Parameter Spaces**: Pre-configured hyperparameter ranges for 10+ algorithms
- ğŸ”„ **YAML Integration**: Seamlessly integrates with existing configuration system
- ğŸ“ˆ **TPE Sampler**: Uses Tree-structured Parzen Estimator for efficient optimization

### ğŸ“Š Evidently Monitor Plugin
Production-ready data drift monitoring:

```bash
# Run monitoring demo with synthetic drift
python plugin_manager.py monitor-demo --reference-data data/your_data.csv --target your_target --drift-severity medium

# Set up production monitoring
python plugin_manager.py cron-setup --reference-data data/training_data.csv --current-data data/production_data.csv
```

**Features:**
- ğŸš¨ **Drift Detection**: Statistical tests for data and target drift
- ğŸ“Š **HTML Reports**: Beautiful, interactive drift analysis reports  
- ğŸ“± **Slack Integration**: Real-time alerts when drift is detected
- ğŸ­ **Synthetic Demos**: Test monitoring with artificially generated drift
- â° **Cron Support**: Schedule automated monitoring checks
- ğŸ“ˆ **Performance Tracking**: Monitor model performance degradation

**Setup Instructions:**
1. Both plugins are pre-configured in `config/unified_config_v3.yaml`
2. Dependencies are included in the project (`optuna`, `evidently`)
3. Use `plugin_manager.py` CLI for all plugin operations

ğŸ“– **Complete Guide:** See [PLUGINS_README.md](PLUGINS_README.md) for detailed documentation

---

## ğŸ” Enhanced Data Quality System

**Intelligent multi-stage data analysis with pattern recognition and quality scoring**

DS-AutoAdvisor includes a comprehensive **Enhanced Data Quality System** that goes beyond basic pandas profiling to provide intelligent data type inference, business pattern detection, and multi-dimensional quality assessment.

### ğŸ¯ Core Capabilities

#### **Multi-Stage Data Type Inference**
**Method:** Heuristic classification with confidence scoring

```
Stage 1: Statistical Analysis â†’ Basic type detection (numeric, text, datetime)
    â†“
Stage 2: Pattern Recognition â†’ Business patterns (email, phone, ID, currency)
    â†“  
Stage 3: Business Rules â†’ Domain validation and compliance checking
    â†“
Stage 4: Cross-Validation â†’ Confidence scoring and final classification
```

**Supported Data Types:**
- **Numeric:** `integer`, `float`, `currency`, `percentage` 
- **Categorical:** `low_cardinality`, `high_cardinality`, `ordinal`
- **Business:** `email`, `phone`, `ssn`, `credit_card`, `uuid`
- **Temporal:** `date`, `datetime`, `timestamp`
- **Identifiers:** `sequential_id`, `composite_key`

#### **Advanced Pattern Detection**

**Business Patterns:**
```python
# Email detection
"user@company.com" â†’ business_email (confidence: 0.95)

# Phone number recognition  
"+1-555-123-4567" â†’ phone_number (confidence: 0.85)

# Currency formatting
"$1,234.56" â†’ currency (confidence: 0.90)

# UUID identification
"123e4567-e89b-12d3-a456-426614174000" â†’ uuid (confidence: 0.98)
```

**Statistical Patterns:**
- Sequential ID detection (1, 2, 3, 4...)
- Distribution analysis (normal, skewed, bimodal)
- Outlier identification (IQR, Z-score methods)
- Correlation analysis between columns

#### **6-Dimensional Quality Assessment**

| Dimension | Weight | What It Measures | Example Issue |
|-----------|--------|------------------|---------------|
| **Completeness** | 25% | Missing value ratio | 15% of emails are null |
| **Consistency** | 20% | Format standardization | Mixed date formats |
| **Accuracy** | 20% | Type correctness | Numbers stored as text |
| **Validity** | 15% | Business rule compliance | Invalid email domains |
| **Uniqueness** | 10% | Duplicate detection | Repeated customer IDs |
| **Timeliness** | 10% | Date range validity | Future birth dates |

**Quality Score Calculation:**
```python
# Weighted scoring across dimensions
overall_score = (
    completeness * 0.25 +
    consistency * 0.20 + 
    accuracy * 0.20 +
    validity * 0.15 +
    uniqueness * 0.10 +
    timeliness * 0.10
)
```

### ğŸ”§ Configuration & Usage

#### **Enable/Disable Components**
```yaml
# config/unified_config_v3.yaml
enhanced_quality_system:
  components:
    enable_type_inference: true      # Multi-stage type detection
    enable_pattern_detection: true   # Business pattern recognition  
    enable_quality_metrics: true     # 6-dimension quality scoring
    enable_cross_validation: true    # Cross-component validation
    
  reporting:
    generate_detailed_report: true   # Full JSON analysis report
    include_recommendations: true    # Actionable improvement suggestions
    include_sample_data: true        # Example values for patterns
    max_sample_size: 100            # Limit sample data size
```

#### **Quality Thresholds**
```yaml
thresholds:
  critical_score_threshold: 40      # Data needs immediate attention
  warning_score_threshold: 60       # Data needs improvement  
  good_score_threshold: 85          # Data is in good condition
```

#### **Pattern Detection Settings**
```yaml
pattern_detection:
  business_patterns:
    email:
      pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
      confidence_threshold: 0.8
    phone:
      pattern: '^[\\+]?[1-9]?[0-9]{7,15}$'
      confidence_threshold: 0.7
  
  anomaly_detection:
    enable_outlier_detection: true
    outlier_methods: ['iqr', 'zscore']
    outlier_threshold: 3.0
```

### ğŸ“Š Output Analysis

#### **Enhanced Quality Report**
Location: `pipeline_outputs/01_discovery_*/reports/enhanced_quality_report.json`

```json
{
  "metadata": {
    "analysis_timestamp": "2025-01-15T10:30:00Z",
    "dataset_shape": [1000, 15],
    "analysis_duration_seconds": 23.4
  },
  "overall_assessment": {
    "quality_score": 73.2,
    "quality_grade": "Good",
    "critical_issues": 2,
    "warning_issues": 5,
    "recommendations_count": 8
  },
  "column_analysis": {
    "customer_email": {
      "inferred_type": "business_email",
      "confidence": 0.95,
      "quality_score": 89.4,
      "patterns_detected": ["email_format"],
      "quality_issues": [],
      "recommendations": [
        "Validate email domains against business rules",
        "Consider standardizing email case"
      ],
      "sample_values": ["user@company.com", "admin@business.org"]
    },
    "phone_number": {
      "inferred_type": "phone",
      "confidence": 0.82,
      "quality_score": 67.1,
      "patterns_detected": ["phone_format"],
      "quality_issues": [
        "15% of values missing area codes",
        "Mixed international/domestic formats"
      ],
      "recommendations": [
        "Standardize phone number format",
        "Implement format validation"
      ]
    }
  },
  "quality_dimensions": {
    "completeness": {
      "score": 85.2,
      "issues": ["phone_number: 8% missing", "address: 12% missing"]
    },
    "consistency": {
      "score": 67.8, 
      "issues": ["date_format: 3 different formats detected"]
    },
    "accuracy": {
      "score": 92.1,
      "issues": ["salary: stored as text with currency symbols"]
    }
  }
}
```

#### **Data Profiling Integration**
The enhanced system integrates with ydata-profiling to create comprehensive reports:

- **`data_profiling_report.html`** - Interactive visual dashboard
- **`raw_profiling_data.json`** - Machine-readable profiling data
- **`enhanced_quality_report.json`** - Advanced quality analysis

### ğŸ¯ Quality-Driven Workflows

#### **High-Quality Data (Score: 85-100)**
```bash
# Minimal intervention needed
python 01_data_discovery.py --data data/clean_data.csv
# Review quality report â†’ proceed with light cleaning
python 02_stage_testing.py --mode fast --stage all
```

#### **Moderate-Quality Data (Score: 60-84)**  
```bash
# Standard cleaning workflow
python 01_data_discovery.py --data data/standard_data.csv
# Review quality report â†’ customize cleaning template
# Edit: cleaning_config_template.yaml based on recommendations
python 02_stage_testing.py --stage cleaning  # Test cleaning
python 02_stage_testing.py --stage training
```

#### **Poor-Quality Data (Score: <60)**
```bash
# Extensive cleaning required
python 01_data_discovery.py --data data/messy_data.csv
# Analyze quality report thoroughly
# Multiple cleaning iterations:
python 02_stage_testing.py --stage cleaning
# Edit cleaning template based on results
python 02_stage_testing.py --stage cleaning  # Test again
# Repeat until quality improves
```

### ğŸ’¡ Best Practices

#### **Interpreting Quality Scores**
- **90-100:** Production-ready data
- **80-89:** Minor issues, proceed with caution
- **70-79:** Moderate cleaning needed
- **60-69:** Significant data quality work required
- **<60:** Extensive data remediation needed

#### **Using Pattern Detection Results**
```yaml
# Email patterns â†’ Use label encoding (not one-hot)
email_column:
  encoding: 'label'
  
# Currency patterns â†’ Extract numeric values  
price_column:
  transformation: 'extract_numeric'
  scaling: 'standard'
  
# Phone patterns â†’ Standardize format first
phone_column:
  text_cleaning: true
  pattern_standardization: 'phone'
  
# ID patterns â†’ Exclude from modeling
customer_id:
  exclude_from_modeling: true
```

#### **Quality Monitoring**
```python
# Track quality scores over time
quality_scores = []
for dataset in datasets:
    score = run_quality_analysis(dataset)
    quality_scores.append(score)
    
# Alert on quality degradation
if current_score < baseline_score * 0.9:
    send_quality_alert()
```

---

## The 3-Step Workflow

### ğŸ” Step 1: Data Discovery & Configuration
**Script:** `01_data_discovery.py`  
**Purpose:** Deep one-time analysis that creates reusable configurations  
**Time:** 3-5 minutes per dataset

```bash
# Basic usage
python 01_data_discovery.py

# With your data
python 01_data_discovery.py --data data/bank.csv --target deposit

# Skip human review (for automation)
python 01_data_discovery.py --auto-approve

# Enhanced profiling (more statistical details)
python 01_data_discovery.py --enhanced-profiling
```

**What it does:**
- ğŸ“Š Comprehensive data profiling (missing values, distributions, correlations)
- ğŸ¯ Automatic target column detection
- ğŸ“ Data quality assessment with scores
- âš™ï¸ **Generates cleaning template** (the key output!)
- ğŸ’¾ Caches expensive computations for reuse
- ğŸ” **Enhanced Quality System** - Multi-stage data type inference with pattern detection
- ğŸ“Š **6-Dimensional Quality Scoring** - Completeness, consistency, accuracy, validity, uniqueness, timeliness
- ğŸ¯ **Business Pattern Recognition** - Emails, phones, IDs, currency, dates with confidence scores

**Key Output:** `cleaning_config_template.yaml` - THIS IS WHAT YOU EDIT!

### âœï¸ CRUCIAL MANUAL STEP: Edit Your Cleaning Template

**Location:** `pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml`

This YAML file controls every data transformation. Example:
```yaml
column_transformations:
  age:
    imputation: 'median'           # How to handle missing values
    outlier_treatment: 'cap'       # How to handle outliers  
    scaling: 'standard'            # How to scale values
    
  salary:
    imputation: 'median'
    outlier_treatment: 'remove'    # Remove extreme salary outliers
    transformation: 'log'          # Log transform skewed salary
    scaling: 'robust'              # Robust scaling for financial data
    
  job_category:
    encoding: 'onehot'             # One-hot encode categories
    text_cleaning: true            # Clean text first
```

**ğŸ’¡ Why This Matters:**
- You have full control over every transformation
- Different datasets need different approaches
- Decisions are documented and reproducible
- You can version control your cleaning logic

### ğŸ§ª Step 2: Stage Testing (Iterative Development)
**Script:** `02_stage_testing.py`  
**Purpose:** Fast testing of individual pipeline components  
**Time:** 30 seconds - 10 minutes depending on scope

```bash
# Test individual stages
python 02_stage_testing.py --stage cleaning      # Test data cleaning only
python 02_stage_testing.py --stage training      # Test model training only
python 02_stage_testing.py --stage evaluation    # Test model evaluation only

# Test multiple stages
python 02_stage_testing.py --stage cleaning,training
python 02_stage_testing.py --stage all

# Use specific discovery results  
python 02_stage_testing.py --stage all --discovery-dir pipeline_outputs/01_discovery_bank_20250811_140000

# Choose mode (fast = fewer models, custom = full control)
python 02_stage_testing.py --mode fast --stage training
```

**Available Stages:**
- **cleaning:** Apply your YAML cleaning rules and see results
- **advisory:** Get ML model recommendations based on your data
- **training:** Train multiple models (limited set for speed)
- **evaluation:** Generate performance metrics and comparisons

**Key Output:** `training_report.json` with model rankings and performance

### ğŸ“¦ Step 3: Production Packaging (No Retraining)
**Script:** `03_full_pipeline.py`  
**Purpose:** Package your best models with deployment assets  
**Time:** 1-3 minutes

```bash
# Auto-detect latest Stage 2 results
python 03_full_pipeline.py

# Use specific Stage 2 results
python 03_full_pipeline.py --stage2-dir pipeline_outputs/02_stage_testing_custom_20250811_150000

# Enable MLflow experiment tracking
python 03_full_pipeline.py --enable-mlflow
```

**What it does:**
- ğŸ“¦ Packages top 3 models from Stage 2
- ğŸ Creates model serving script (`model_server.py`)
- ğŸŒ Generates API template (`api_template.py`)
- ğŸ³ Creates Dockerfile for deployment
- ğŸ“– Writes executive summary and deployment docs
- ğŸ“Š Generates model comparison reports

**Key Design Choice:** No retraining! This step packages existing models for speed.

---
## Important Manual Steps

### ğŸ›‘ Step 1.5: Edit cleaning_config_template.yaml (REQUIRED)

After Step 1, you MUST review and customize the cleaning template:

1. **Find the file:**
   ```bash
   ls pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml
   ```

2. **Key decisions to make for each column:**
   - **Missing values:** median, mean, mode, or custom value?
   - **Outliers:** remove, cap, or keep?
   - **Encoding:** one-hot, label, target, or ordinal?
   - **Scaling:** standard, minmax, or robust?
   - **Transformations:** log, sqrt, or none?

3. **Common patterns:**
   ```yaml
   # Numeric columns with outliers
   salary:
     imputation: 'median'
     outlier_treatment: 'cap'     # Cap at 95th percentile
     transformation: 'log'        # Log transform for skewness
     scaling: 'robust'
   
   # Categorical with few categories  
   gender:
     imputation: 'most_frequent'
     encoding: 'onehot'
   
   # High cardinality categorical
   job_title:
     encoding: 'target'           # Target encoding
     max_categories: 20           # Limit categories
   ```

4. **Test your changes:**
   ```bash
   python 02_stage_testing.py --stage cleaning
   ```

---
## What You Get From Each Step
| Step | Folder | Key Files | What They Contain |
|------|--------|-----------|-------------------|
| **1** | `01_discovery_*` | `configs/cleaning_config_template.yaml`<br>`reports/profiling_report.html`<br>`reports/enhanced_quality_report.json`<br>`data_profiles/data_profiling_report.html`<br>`cache/` | **The cleaning template to edit**<br>Visual data profiling report<br>**Enhanced quality analysis with pattern detection**<br>**Interactive profiling dashboard**<br>Cached computations for reuse |
| **2** | `02_stage_testing_*` | `training_report.json`<br>`models/*.pkl`<br>`cleaned_data/` | Model rankings and metrics<br>Trained model files<br>Cleaned datasets (using your rules) |
| **3** | `03_production_artifacts_*` | `models/` (packaged)<br>`deployment/model_server.py`<br>`deployment/api_template.py`<br>`documentation/` | Production-ready models<br>Flask serving script<br>REST API template<br>Deployment guides |

### ğŸ¯ The Most Important Files

1. **`cleaning_config_template.yaml`** (Step 1) - YOU MUST EDIT THIS
2. **`enhanced_quality_report.json`** (Step 1) - Comprehensive data quality analysis with pattern detection
3. **`training_report.json`** (Step 2) - Contains your model rankings
4. **`models_index.json`** (Step 3) - Lists packaged models and metadata
5. **`model_server.py`** (Step 3) - Ready-to-run model serving script

---
## Common Use Cases

### ğŸ†• New Dataset Workflow
```bash
# First time with new data
python 01_data_discovery.py --data data/new_dataset.csv --target outcome
# Edit: pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml
python 02_stage_testing.py --stage all
python 03_full_pipeline.py --enable-mlflow
```

### ğŸ”„ Iterative Development  
```bash
# You have discovery results, testing cleaning changes
python 02_stage_testing.py --stage cleaning  # Test cleaning changes
# Edit cleaning_config_template.yaml again if needed
python 02_stage_testing.py --stage training  # Test training with new cleaning
python 03_full_pipeline.py                   # Package when satisfied
```

### ğŸš€ Quick Experimentation
```bash
# Test just training with different models
python 02_stage_testing.py --mode fast --stage training
```

### ğŸ­ Production Pipeline
```bash
# Full pipeline with tracking
python 01_data_discovery.py --auto-approve  # Skip reviews
python 02_stage_testing.py --stage all
python 03_full_pipeline.py --enable-mlflow
./mlflow_ui.sh  # View results
```

---
## Configuration Guide

### ğŸ›ï¸ Configuration Priority (High â†’ Low)
1. **Command line arguments** (`--data`, `--target`, etc.)
2. **Discovery-generated configs** (from Step 1)
3. **Main config file** (`config/unified_config_v3.yaml`)
4. **Built-in defaults**

### ğŸ”§ Key Configuration Files

**Main Config:** `config/unified_config_v3.yaml`
```yaml
global:
  data_input_path: "data/your_data.csv"
  target_column: "your_target"

fast_mode:         # Fewer models, faster execution
  training:
    max_models: 5
    
custom_mode:       # Full control, more models
  training:
    max_models: 15
    enable_ensemble: true
```

**Cleaning Config:** `cleaning_config_template.yaml` (generated, then edited by you)
```yaml
column_transformations:
  your_column:
    imputation: 'median'
    outlier_treatment: 'cap'
    encoding: 'onehot'
    scaling: 'standard'
```

---
## MLflow Tracking

MLflow tracks your experiments and models for reproducibility.

### ğŸš€ Enable Tracking
```bash
# Enable for Step 3 (recommended)
python 03_full_pipeline.py --enable-mlflow

# View in browser
./mlflow_ui.sh 5000
# Visit: http://localhost:5000
```

### ğŸ“Š What Gets Tracked
- **Experiments:** Each pipeline run
- **Metrics:** Model accuracy, training time, rankings
- **Parameters:** Model types, cleaning settings
- **Artifacts:** Model files, reports, configurations

### ğŸ¯ Why Use MLflow
- Compare multiple pipeline runs
- Track which cleaning settings work best
- Version your models automatically
- Share results with team

**Tradeoff:** Adds ~10% overhead but provides valuable experiment tracking.

---
## Design Choices & Limitations

### ğŸ¯ Key Design Decisions

| Decision | Why We Did It | Benefit | Limitation | When to Revisit |
|----------|---------------|---------|------------|-----------------|
| **Cache discovery results** | Avoid re-profiling same data | 5x faster iteration | Stale if data changes | Add drift detection |
| **Manual cleaning config** | Full transparency & control | You see every transformation | Extra manual step | Auto-suggest improvements |
| **Limited models in Stage 2** | Fast feedback loops | Test in minutes not hours | Might miss best model | Re-run with more models |
| **No retraining in Step 3** | Ultra-fast packaging | 1-3 min deployment prep | Metrics may be outdated | Add optional retrain flag |
| **Pickle model format** | Simple & fast | Works everywhere | Security/portability risk | Move to MLflow format |
| **Flask API template** | Zero dependencies | Easy to understand | No auth or scaling | Upgrade to FastAPI |

### âš ï¸ Current Limitations

**Data & Processing:**
- No automatic data drift detection
- No streaming/incremental updates
- Single-process execution (no parallelism)
- Basic hyperparameter optimization

**Production & Deployment:**
- No authentication in API template
- No built-in monitoring or alerting
- No automatic model versioning/promotion
- No A/B testing framework

**MLOps & Infrastructure:**
- No job scheduler or orchestration
- No feature store integration
- No automated retraining triggers
- No fairness/bias detection built-in

### ğŸ’¡ When to Use Alternative Tools

**Use DS-AutoAdvisor when:**
- Exploring new datasets quickly
- Need transparent, controllable cleaning
- Want fast iteration cycles
- Building proof-of-concepts

**Consider other tools when:**
- Need production MLOps platform (Kubeflow, MLflow + Airflow)
- Require real-time serving (TensorFlow Serving, Seldon)
- Need advanced AutoML (H2O.ai, DataRobot)
- Have streaming data requirements (Apache Kafka + Flink)

---
## Troubleshooting

### ğŸš¨ Common Issues & Quick Fixes

| Problem | Quick Check | Solution |
|---------|-------------|----------|
| **"No discovery directory found"** | `ls pipeline_outputs/01_discovery_*` | Run Step 1 first |
| **"cleaning_config_template.yaml not found"** | Check if Step 1 completed | Look in `pipeline_outputs/01_discovery_*/configs/` |
| **Models not training** | Check target column name | Verify target exists in your data |
| **MLflow UI empty** | Check `--enable-mlflow` flag | Ensure MLflow enabled in Step 3 |
| **"No models packaged"** | Check `training_report.json` | Verify Stage 2 completed successfully |
| **API script fails** | Check model files exist | Verify models/ directory has .pkl files |
| **Quality score 0.0** | Check enhanced system config | Verify `enhanced_quality_system.enabled: true` in config |
| **Pattern detection not working** | Check pattern configuration | Review `pattern_detection` settings in unified_config_v3.yaml |

### ğŸ” Debugging Steps

1. **Check outputs of previous step:**
   ```bash
   ls pipeline_outputs/*/
   cat pipeline_outputs/*/summary.json
   ```

2. **Test individual stages:**
   ```bash
   python 02_stage_testing.py --stage cleaning  # Test just cleaning
   ```

3. **Check logs:**
   ```bash
   ls logs/
   tail -f logs/audit.log
   ```

4. **Review quality analysis:**
   ```bash
   cat pipeline_outputs/01_discovery_*/reports/enhanced_quality_report.json
   # Check overall_assessment.quality_score and critical_issues
   ```

### ğŸ’¾ File Locations Cheat Sheet

```
ğŸ“ Project Structure:
â”œâ”€â”€ pipeline_outputs/           # All your results
â”‚   â”œâ”€â”€ 01_discovery_*/        # Step 1 outputs
â”‚   â”œâ”€â”€ 02_stage_testing_*/    # Step 2 outputs  
â”‚   â””â”€â”€ 03_production_*/       # Step 3 outputs
â”œâ”€â”€ config/                    # Configuration files
â”œâ”€â”€ logs/                      # Execution logs
â”œâ”€â”€ mlruns/                    # MLflow tracking
â””â”€â”€ data/                      # Your input data
```

---
## Roadmap (Future Features)

### ğŸ¯ Short Term (Next Release)
- **Optional retraining in Step 3** (for up-to-date metrics)
- **Enhanced plugin system** (more monitoring metrics, additional HPO algorithms)
- **Advanced deployment templates** (FastAPI, containerization)
- **Real-time monitoring dashboards** (live drift visualization)

### ğŸš€ Medium Term (6 months)
- **Model registry integration** (automatic versioning/promotion)
- **Fairness & bias detection** (ethical AI metrics)
- **Feature importance caching** (SHAP explanations)
- **CI/CD templates** (GitHub Actions workflows)
- **Cloud monitoring integration** (AWS CloudWatch, Azure Monitor)

### ğŸ­ Long Term (1 year)
- **Streaming data support** (Kafka integration)
- **Feature store abstraction** (Feast integration)
- **Advanced production monitoring** (model performance tracking)
- **ONNX export support** (cross-platform deployment)
- **Security hardening** (auth, input validation, audit)
- **Parallel processing** (multi-core/distributed execution)

### ğŸ¤ Community Features
- **Extended plugin marketplace** (community-contributed plugins)
- **Model marketplace** (pre-trained models)
- **Template library** (industry-specific configs)
- **Integration gallery** (cloud platforms, databases)

### âœ… Recently Completed
- **ğŸ¯ Optuna HPO Plugin** - Intelligent hyperparameter optimization with auto-config generation
- **ğŸ“Š Evidently Monitor Plugin** - Data drift detection and performance monitoring
- **ğŸ”§ Plugin Manager CLI** - Easy plugin management and configuration
- **ğŸ“± Slack Integration** - Real-time monitoring alerts
- **ğŸ­ Synthetic Drift Demo** - Testing capabilities with artificial drift

---
## Summary: Why This Design Works

âœ… **Transparent:** You see and control every transformation  
âœ… **Fast:** Cache expensive operations, test components separately  
âœ… **Reproducible:** Version-controlled configs, MLflow tracking  
âœ… **Production-ready:** Deployment scripts, docs, APIs included  
âœ… **Extensible:** Easy to add stages, models, or deployment targets  
âœ… **Enhanced:** Advanced HPO and monitoring capabilities out-of-the-box
âœ… **Intelligent:** Multi-stage data type inference with business pattern recognition
âœ… **Quality-Driven:** 6-dimensional quality assessment guides data preparation decisions

**Perfect for:** Data scientists who want fast iteration with full control over their ML pipeline, enhanced with production-grade optimization, monitoring, and intelligent data quality analysis.

---

**ğŸ‰ Ready to start?** Jump to [Quick Start](#quick-start) and turn your data into models in 15 minutes!