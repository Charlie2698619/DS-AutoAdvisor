# DS-AutoAdvisor Version 3.0 Simplified Configuration
# Two-Mode System: FAST and CUSTOM
# Complete YAML control - no hardcoded settings in Python scripts

# =============================================================================
# ðŸŽ¯ GLOBAL SETTINGS (shared by both modes)
# =============================================================================
global:
  project_name: "DS-AutoAdvisor Pipeline"           # Project name for tracking
  version: "3.0"                                    # Pipeline version
  environment: "development"                        # Options: "development", "staging", "production"
  data_input_path: "data/telco_churn_data.csv"     # Default input data path
  target_column: "Churn"                            # Primary target column name
  target_column_variants: ["Churn", "Churn_binary_0", "Churn_encoded", "Churn_label"]  # Alternative target names
  csv_delimiter: ","                                # CSV delimiter: ",", ";", "\t", "|"
  csv_encoding: "utf-8"                            # File encoding: "utf-8", "latin1", "cp1252"
  random_state: 42                                  # Global random seed for reproducibility
  output_base_dir: "pipeline_outputs"              # Base directory for all outputs
  versioning_enabled: true                         # Enable output versioning (true/false)

# =============================================================================
# ðŸ”§ ENHANCED QUALITY SYSTEM (shared by both modes)
# =============================================================================
enhanced_quality_system:
  components:
    enable_type_inference: true               # Enable multi-stage data type inference (true/false)
    enable_pattern_detection: true            # Enable advanced pattern detection (true/false)
    enable_quality_metrics: true              # Enable comprehensive quality metrics (true/false)
    enable_cross_validation: true             # Enable cross-component validation (true/false)
  reporting:
    generate_detailed_report: true            # Generate detailed JSON report (true/false)
    include_recommendations: true             # Include actionable recommendations (true/false)
    include_sample_data: true                 # Include sample data in reports (true/false)
    max_sample_size: 100                      # Maximum sample size for examples (50-500)
  thresholds:
    critical_score_threshold: 40              # Score below which data is critical (20-60)
    warning_score_threshold: 60               # Score below which warnings are issued (60-80)
    good_score_threshold: 85                  # Score above which data is considered good (80-95)
  integration:
    use_yaml_config: true                     # Use YAML configuration (true/false)
    respect_mode_settings: true               # Respect FAST/CUSTOM mode settings (true/false)
    output_format: "comprehensive"            # Output format: "basic", "detailed", "comprehensive"

# === DATA TYPE INFERENCE CONFIGURATION ===
data_type_inference:
  confidence_thresholds:
    high: 0.8                                 # High confidence threshold (0.7-0.95)
    medium: 0.6                               # Medium confidence threshold (0.5-0.8)
    low: 0.4                                  # Low confidence threshold (0.2-0.6)
  cardinality_thresholds:
    unique_ratio_for_id: 0.95                # Unique ratio to classify as ID (0.9-0.99)
    unique_ratio_for_categorical: 0.5        # Unique ratio for categorical (0.3-0.7)
    max_categories: 50                        # Max categories for categorical (20-100)
  datetime_patterns:
    - '\d{4}-\d{2}-\d{2}'                 # YYYY-MM-DD format
    - '\d{2}/\d{2}/\d{4}'                 # MM/DD/YYYY format
    - '\d{2}-\d{2}-\d{4}'                 # MM-DD-YYYY format
    - '\d{4}/\d{2}/\d{2}'                 # YYYY/MM/DD format
  numeric_patterns:
    - '^[-+]?\d*\.?\d+([eE][-+]?\d+)?$' # Scientific notation
    - '^[-+]?\d+$'                         # Integer
    - '^[-+]?\d*\.\d+$'                  # Decimal
  statistical_tests:
    enable_normality_test: true              # Enable normality testing (true/false)
    enable_chi_square_test: true             # Enable chi-square testing (true/false)
    sample_size_for_tests: 1000              # Sample size for statistical tests (500-5000)

# === PATTERN DETECTION CONFIGURATION ===
pattern_detection:
  business_patterns:
    email:
      pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
      confidence_threshold: 0.8
      description: 'Email address format'
    phone:
      pattern: '^[\+]?[1-9]?[0-9]{7,15}$'
      confidence_threshold: 0.7
      description: 'Phone number format'
    ssn:
      pattern: '^\d{3}-\d{2}-\d{4}$'
      confidence_threshold: 0.9
      description: 'Social Security Number'
    credit_card:
      pattern: '^\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}$'
      confidence_threshold: 0.8
      description: 'Credit card number'
  formatting_patterns:
    currency:
      pattern: '^\$?\d{1,3}(,\d{3})*(\.\d{2})?$'
      confidence_threshold: 0.7
      description: 'Currency format'
    percentage:
      pattern: '^\d{1,3}(\.\d+)?%$'
      confidence_threshold: 0.8
      description: 'Percentage format'
    zip_code:
      pattern: '^\d{5}(-\d{4})?$'
      confidence_threshold: 0.8
      description: 'ZIP code format'
    iso_date:
      pattern: '^\d{4}-\d{2}-\d{2}$'
      confidence_threshold: 0.9
      description: 'ISO date format (YYYY-MM-DD)'
  id_patterns:
    uuid:
      pattern: '^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
      confidence_threshold: 0.95
      description: 'UUID format'
    sequential_id:
      pattern: 'SEQUENTIAL_CHECK'
      confidence_threshold: 0.8
      description: 'Sequential identifier'
  anomaly_detection:
    enable_outlier_detection: true           # Enable outlier detection (true/false)
    outlier_methods: ['iqr', 'zscore']       # Outlier detection methods
    outlier_threshold: 3.0                   # Outlier threshold (2.0-5.0)
    enable_inconsistency_detection: true     # Enable inconsistency detection (true/false)
  statistical_patterns:
    enable_distribution_analysis: true       # Enable distribution analysis (true/false)
    enable_correlation_analysis: true        # Enable correlation analysis (true/false)
    correlation_threshold: 0.8               # Correlation threshold (0.7-0.95)

# === QUALITY METRICS CONFIGURATION ===
quality_metrics:
  dimensions:
    completeness:
      weight: 0.25                            # Weight for completeness dimension (0.1-0.4)
      thresholds:
        excellent: 0.95                       # Excellent threshold (0.9-0.99)
        good: 0.85                           # Good threshold (0.8-0.9)
        acceptable: 0.70                      # Acceptable threshold (0.6-0.8)
        poor: 0.50                           # Poor threshold (0.4-0.6)
    consistency:
      weight: 0.20                            # Weight for consistency dimension (0.1-0.3)
      thresholds:
        excellent: 0.98                       # Excellent threshold (0.95-0.99)
        good: 0.90                           # Good threshold (0.85-0.95)
        acceptable: 0.75                      # Acceptable threshold (0.7-0.85)
        poor: 0.60                           # Poor threshold (0.5-0.7)
    accuracy:
      weight: 0.20                            # Weight for accuracy dimension (0.1-0.3)
      thresholds:
        excellent: 0.95                       # Excellent threshold (0.9-0.99)
        good: 0.85                           # Good threshold (0.8-0.9)
        acceptable: 0.70                      # Acceptable threshold (0.6-0.8)
        poor: 0.55                           # Poor threshold (0.4-0.6)
    validity:
      weight: 0.15                            # Weight for validity dimension (0.1-0.25)
      thresholds:
        excellent: 0.98                       # Excellent threshold (0.95-0.99)
        good: 0.90                           # Good threshold (0.85-0.95)
        acceptable: 0.80                      # Acceptable threshold (0.7-0.9)
        poor: 0.65                           # Poor threshold (0.5-0.7)
    uniqueness:
      weight: 0.10                            # Weight for uniqueness dimension (0.05-0.2)
      thresholds:
        excellent: 0.99                       # Excellent threshold (0.95-1.0)
        good: 0.95                           # Good threshold (0.9-0.98)
        acceptable: 0.85                      # Acceptable threshold (0.8-0.9)
        poor: 0.70                           # Poor threshold (0.6-0.8)
    timeliness:
      weight: 0.10                            # Weight for timeliness dimension (0.05-0.2)
      thresholds:
        excellent: 0.95                       # Excellent threshold (0.9-1.0)
        good: 0.85                           # Good threshold (0.8-0.9)
        acceptable: 0.70                      # Acceptable threshold (0.6-0.8)
        poor: 0.50                           # Poor threshold (0.4-0.6)
  scoring:
    excellent_score: 100                      # Score for excellent quality (95-100)
    good_score: 80                           # Score for good quality (75-85)
    acceptable_score: 60                      # Score for acceptable quality (55-65)
    poor_score: 40                           # Score for poor quality (30-45)
    failing_score: 20                        # Score for failing quality (10-25)
  validation_rules:
    numeric_ranges: true                     # Enable numeric range validation (true/false)
    categorical_domains: true                # Enable categorical domain validation (true/false)
    date_ranges: true                        # Enable date range validation (true/false)
    business_rules: true                     # Enable business rule validation (true/false)

# === DATA CLEANING CONFIGURATION (global defaults) ===
data_cleaning:
  outlier_detection:
    outlier_removal: true                    # Remove outliers (true/false)
    outlier_method: "iqr"                    # Default method: "iqr", "isoforest", "zscore", "lof", "elliptic_envelope", "ensemble"
    iqr_factor: 1.5                          # IQR factor for outlier detection (1.0-3.0)
    iforest_contam: 0.01                     # Isolation Forest contamination rate (0.001-0.1)
    zscore_thresh: 3.0                       # Z-score threshold (2.0-5.0)
    lof_n_neighbors: 20                      # LOF number of neighbors (5-50)
    lof_contamination: 0.01                  # LOF contamination rate (0.001-0.1)
    elliptic_contamination: 0.01             # Elliptic Envelope contamination rate (0.001-0.1)
    elliptic_support_fraction: null          # Elliptic Envelope support fraction (null=auto, 0.1-1.0)
    ensemble_methods: ['iqr', 'isoforest', 'zscore']  # Methods for ensemble outlier detection
    ensemble_voting: "union"                 # Ensemble voting: "union", "intersection", "majority"
    ensemble_contamination: 0.05             # Overall contamination for ensemble (0.001-0.2)

# =============================================================================
# ðŸš€ FAST MODE - Quick pipeline execution with minimal configuration
# =============================================================================
fast_mode:
  # Data Discovery Settings
  data_discovery:
    # === PROFILING CONFIGURATION (FAST) ===
    profiling:
      output_dir_name: "data_profiles"               # Directory name for profiling outputs
      generate_html: true                            # Generate interactive HTML reports (true/false)
      save_raw_profile: false                        # Save raw profiling JSON data (true/false) - FAST: disabled
      enable_advanced_stats: false                   # Enable advanced statistical analysis (true/false) - FAST: disabled
      generate_data_lineage: false                   # Generate data lineage tracking (true/false) - FAST: disabled
      
    # === QUALITY ASSESSMENT (FAST) ===
    quality_assessment:
      enabled: true                                  # Enable data quality assessment (true/false)
      use_enhanced_system: false                     # Use enhanced quality system (true/false) - FAST: basic
      quality_gates: false                           # Enable quality gates (true/false) - FAST: disabled
      automated_recommendations: true               # Generate automated recommendations (true/false)
      thresholds:
        completeness_min: 0.7                       # Min completeness threshold (0.5-0.99) - FAST: relaxed
        consistency_min: 0.6                        # Min consistency threshold (0.5-0.95) - FAST: relaxed
        validity_min: 0.8                           # Min validity threshold (0.6-0.99) - FAST: relaxed
        uniqueness_check: false                      # Enable uniqueness checking (true/false) - FAST: disabled
        
    # === CONFIGURATION GENERATION (FAST) ===
    config_generation:
      cleaning_config_template: true                # Generate cleaning config template (true/false)
      pipeline_config_update: false                 # Update pipeline config (true/false) - FAST: disabled
      auto_detect_target: true                      # Auto-detect target column (true/false)
      suggest_improvements: false                    # Suggest data improvements (true/false) - FAST: disabled

  # Data Cleaning Settings (FAST MODE)
  data_cleaning:
    # === EXECUTION CONFIGURATION (FAST) ===
    execution:
      validate_before_cleaning: false               # Validate data before cleaning (true/false) - FAST: disabled
      backup_original_data: false                   # Create backup of original data (true/false) - FAST: disabled
      generate_cleaning_report: false              # Generate detailed cleaning report (true/false) - FAST: disabled
      save_intermediate_steps: false               # Save intermediate cleaning steps (true/false) - FAST: disabled
      
    # === QUALITY GATES (FAST) ===
    quality_gates:
      enabled: false                                # Enable quality gates (true/false) - FAST: disabled
      min_data_retention: 0.3                      # Minimum data retention ratio (0.3-0.9) - FAST: relaxed
      max_missing_after_cleaning: 0.2              # Max missing values after cleaning (0.05-0.3) - FAST: relaxed
      require_target_column: true                   # Require target column presence (true/false)
      
    # === OUTPUT CONFIGURATION (FAST) ===
    output:
      save_cleaned_data: true                       # Save cleaned dataset (true/false)
      output_filename: "cleaned_data.csv"          # Output filename
      log_all_transformations: false               # Log all transformation steps (true/false) - FAST: disabled
      generate_comparison_report: false            # Generate before/after comparison (true/false) - FAST: disabled

  # ML Advisory Settings (FAST MODE)
  ml_advisory:
    # === ASSUMPTION TESTING CONFIGURATION (FAST) ===
    assumption_testing:
      enabled: true                                    # Enable assumption testing (true/false)
      normality_alpha: 0.05                          # Significance level for normality tests (0.01-0.1)
      homo_alpha: 0.05                               # Significance level for variance tests (0.01-0.1)
      vif_threshold: 10.0                            # VIF threshold for multicollinearity (5.0-50.0)
      correlation_threshold: 0.95                    # Correlation threshold (0.8-0.99)
      verbose: false                                 # Verbose assumption testing output (true/false) - FAST: disabled
      generate_recommendations: false               # Generate model recommendations (true/false) - FAST: disabled
      normality_max_sample: 1000                    # Max samples for normality testing (1000-10000) - FAST: limited
      normality_method: "shapiro"                    # Options: "shapiro", "kolmogorov", "anderson", "jarque_bera"
      homo_method: "breusch_pagan"                   # Options: "breusch_pagan", "goldfeld_quandt", "white"
      imbalance_threshold: 0.9                       # Threshold for class imbalance (0.6-0.95)
      min_class_size: 30                             # Minimum class size for analysis (10-100)
      linearity_alpha: 0.05                         # Significance level for linearity tests (0.01-0.1)
      independence_alpha: 0.05                       # Significance level for independence tests (0.01-0.1)
      chunk_size: 1000                               # Chunk size for large datasets (1000-50000) - FAST: smaller chunks
      enable_sampling: true                          # Enable sampling for large datasets (true/false)
      max_features_vif: 20                           # Max features for VIF calculation (10-100) - FAST: limited
      
    # === MODEL RECOMMENDATION SETTINGS (FAST) ===
    model_recommendation:
      enabled: false                                 # Enable model recommendations (true/false) - FAST: disabled
      include_ensemble_suggestions: false           # Include ensemble recommendations (true/false) - FAST: disabled
      consider_dataset_size: true                   # Consider dataset size in recommendations (true/false)
      performance_vs_interpretability: "performance" # Options: "performance", "interpretability", "balanced" - FAST: performance focus
      
    # === META-LEARNING CONFIGURATION (FAST) ===
    meta_learning:
      enabled: false                                 # Enable meta-learning features (true/false) - FAST: disabled
      performance_database: "metadata/model_performance.db"  # Path to performance database
      schema_similarity_threshold: 0.8              # Schema similarity threshold (0.5-0.95)
      min_historical_samples: 5                     # Min samples for meta-learning (3-20)
      transfer_learning: false                       # Enable transfer learning (true/false) - FAST: disabled
      
    # === FAIRNESS & BIAS DETECTION (FAST) ===
    fairness:
      enabled: false                                 # Enable fairness analysis (true/false) - FAST: disabled
      protected_attributes: []                       # List of protected attributes (e.g., ["gender", "race"])
      fairness_metrics: []                          # Fairness metrics to compute - FAST: none
      bias_threshold: 0.1                           # Bias detection threshold (0.05-0.2)
      mitigation_strategies: []                      # Available mitigation strategies - FAST: none

  # Model Training Settings (FAST MODE)
  model_training:
    # === DATA SPLITTING CONFIGURATION (FAST) ===
    test_size: 0.3                      # Split ratio for test set (0.1-0.4) - FAST: larger test set
    validation_size: 0.0                # Split ratio for validation set (0.0-0.3) - FAST: no validation
    random_state: 42                    # Random seed for reproducibility
    
    # === MODEL SELECTION (FAST) ===
    max_models: 3                       # Maximum number of models to train (1-20) - FAST: limited
    include_ensemble: false             # Include ensemble methods (true/false) - FAST: disabled
    include_advanced: false             # Include advanced algorithms (true/false) - FAST: disabled
    models_to_use: null                 # Specific models to use (null for auto-select) - FAST: auto-select basic models
    
    # === HYPERPARAMETER TUNING (FAST) ===
    enable_tuning: false                # Enable hyperparameter optimization (true/false) - FAST: disabled
    tuning_method: "none"               # Options: "grid", "random", "bayesian", "optuna", "none" - FAST: none
    tuning_iterations: 10               # Number of tuning iterations (10-500) - FAST: minimal
    tuning_cv_folds: 3                  # CV folds for tuning (3-10) - FAST: minimal
    
    # === OPTUNA HPO SETTINGS (FAST) ===
    optuna_hpo_settings:
      sampler: "TPE"                    # Optuna sampler: "TPE", "Random", "CmaEs" - FAST: TPE
      pruner: "MedianPruner"           # Optuna pruner: "MedianPruner", "Hyperband", "SuccessiveHalving" - FAST: MedianPruner
      direction: "maximize"             # Optimization direction: "maximize", "minimize" - FAST: maximize
      n_startup_trials: 5               # Number of startup trials (5-20) - FAST: minimal
      n_warmup_steps: 5                # Number of warmup steps (5-20) - FAST: minimal
      timeout_minutes: 10               # HPO timeout in minutes (5-60) - FAST: short
      study_name_suffix: "fast"         # Study name suffix - FAST: descriptive
    
    # === VALIDATION CONFIGURATION (FAST) ===
    cv_folds: 3                         # Cross-validation folds (3-10) - FAST: minimal
    scoring_strategy: "fast"            # Options: "fast", "comprehensive", "custom" - FAST: fast scoring
    
    # === PERFORMANCE SETTINGS (FAST) ===
    parallel_jobs: 1                    # Parallel jobs (-1=all cores, 1=single, 2-16=specific) - FAST: single core
    max_training_time_minutes: 10       # Max training time limit (5-120 minutes, null=no limit) - FAST: short time limit
    memory_limit_gb: null               # Memory limit in GB (1-32, null=no limit)
    
    # === PREPROCESSING OPTIONS (FAST) ===
    encoding_strategy: "none"           # Options: "none", "auto", "onehot", "target", "binary" - FAST: none
    scaling_strategy: "none"            # Options: "none", "standard", "minmax", "robust", "quantile" - FAST: none
    
    # === BUSINESS FEATURES (FAST) ===
    enable_business_features: false     # Enable business-specific features (true/false) - FAST: disabled
    feature_selection_enabled: false   # Enable automated feature selection (true/false) - FAST: disabled
    business_kpi_tracking: false       # Track business KPIs during training (true/false) - FAST: disabled
    business_rules_file: "config/business_rules.yaml"  # Path to business rules config
    business_kpis_file: "config/business_kpis.yaml"    # Path to business KPIs config
    
    # === OUTPUT CONFIGURATION (FAST) ===
    save_models: false                  # Save trained models to disk (true/false) - FAST: disabled for speed
    model_dir: "models"                 # Directory for saving models
    save_predictions: false             # Save model predictions (true/false) - FAST: disabled
    verbose: false                      # Verbose output during training (true/false) - FAST: minimal output

  # Model Evaluation Settings (FAST MODE)
  model_evaluation:
    # === INPUT/OUTPUT CONFIGURATION (FAST) ===
    models_dir: "models"                               # Directory containing trained models
    training_report_path: "comprehensive_training_report.json"  # Path to training report
    output_dir: "evaluation_results"                   # Output directory for evaluation results
    
    # === ANALYSIS COMPONENTS (FAST - Most Disabled) ===
    enable_shap: false                                 # SHAP feature importance analysis (true/false) - FAST: disabled
    enable_learning_curves: false                     # Learning curve analysis (true/false) - FAST: disabled
    enable_residual_analysis: false                   # Residual analysis for regression (true/false) - FAST: disabled
    enable_stability_analysis: false                  # Model stability testing (true/false) - FAST: disabled
    enable_interpretability: false                    # Model interpretability analysis (true/false) - FAST: disabled
    enable_uncertainty_analysis: false                # Uncertainty quantification (true/false) - FAST: disabled
    
    # === VISUALIZATION SETTINGS (FAST) ===
    save_plots: false                                  # Save visualization plots (true/false) - FAST: disabled
    plot_format: "png"                                # Options: "html", "png", "svg", "both" - FAST: simple PNG
    plot_dpi: 150                                      # Plot resolution (150-600 DPI) - FAST: lower resolution
    figure_size: [8, 6]                               # Figure size [width, height] in inches - FAST: smaller
    
    # === PERFORMANCE SETTINGS (FAST) ===
    n_permutations: 10                                 # Permutations for feature importance (10-500) - FAST: minimal
    n_bootstrap_samples: 20                           # Bootstrap samples for stability (50-1000) - FAST: minimal
    max_shap_samples: 100                             # Max samples for SHAP analysis (100-5000) - FAST: minimal
    n_models_to_evaluate: 1                           # Number of top models to evaluate (1-10) - FAST: single model
    
    # === FEATURE IMPORTANCE CONFIGURATION (FAST) ===
    feature_importance:
      top_k_features: 5                               # Number of top features to display (5-50) - FAST: fewer features
      show_direction: false                           # Show positive/negative impact (true/false) - FAST: disabled
      add_log_odds_axis: false                        # Add log-odds scale for classification (true/false) - FAST: disabled
      permutation_error_bars: false                   # Show error bars in permutation plots (true/false) - FAST: disabled
      
    # === STABILITY TESTING CONFIGURATION (FAST) ===
    stability_testing:
      noise_levels: [0.05]                            # Noise levels for testing [0.001-0.5] - FAST: single level
      dropout_rates: [0.1]                            # Feature dropout rates [0.01-0.5] - FAST: single rate
      random_state: 42                                # Random seed for reproducibility
      
    # === LOGGING CONFIGURATION (FAST) ===
    verbose: false                                     # Verbose output (true/false) - FAST: minimal output
    log_level: "WARNING"                              # Options: "DEBUG", "INFO", "WARNING", "ERROR" - FAST: warnings only

# =============================================================================
# ðŸ”§ CUSTOM MODE - Full control with comprehensive configuration
# =============================================================================
custom_mode:
  # Data Discovery Settings
  data_discovery:
    # === PROFILING CONFIGURATION ===
    profiling:
      output_dir_name: "data_profiles"               # Directory name for profiling outputs
      generate_html: true                            # Generate interactive HTML reports (true/false)
      save_raw_profile: true                         # Save raw profiling JSON data (true/false)
      enable_advanced_stats: true                    # Enable advanced statistical analysis (true/false)
      generate_data_lineage: true                    # Generate data lineage tracking (true/false)
      
    # === QUALITY ASSESSMENT ===
    quality_assessment:
      enabled: true                                  # Enable data quality assessment (true/false)
      use_enhanced_system: true                      # Use enhanced quality system (true/false)
      quality_gates: true                            # Enable quality gates (true/false)
      automated_recommendations: true               # Generate automated recommendations (true/false)
      thresholds:
        completeness_min: 0.8                       # Min completeness threshold (0.5-0.99)
        consistency_min: 0.7                        # Min consistency threshold (0.5-0.95)
        validity_min: 0.9                           # Min validity threshold (0.6-0.99)
        uniqueness_check: true                       # Enable uniqueness checking (true/false)
      
    # === CONFIGURATION GENERATION ===
    config_generation:
      cleaning_config_template: true                # Generate cleaning config template (true/false)
      pipeline_config_update: true                  # Update pipeline config (true/false)
      auto_detect_target: true                      # Auto-detect target column (true/false)
      suggest_improvements: true                     # Suggest data improvements (true/false)

  # Data Cleaning Settings
  data_cleaning:
    # === EXECUTION CONFIGURATION ===
    execution:
      validate_before_cleaning: true                # Validate data before cleaning (true/false)
      backup_original_data: false                   # Create backup of original data (true/false)
      generate_cleaning_report: true               # Generate detailed cleaning report (true/false)
      save_intermediate_steps: false               # Save intermediate cleaning steps (true/false)
      
    # === QUALITY GATES ===
    quality_gates:
      enabled: true                                 # Enable quality gates (true/false)
      min_data_retention: 0.5                      # Minimum data retention ratio (0.3-0.9)
      max_missing_after_cleaning: 0.1              # Max missing values after cleaning (0.05-0.3)
      require_target_column: true                   # Require target column presence (true/false)
      
    # === OUTPUT CONFIGURATION ===
    output:
      save_cleaned_data: true                       # Save cleaned dataset (true/false)
      output_filename: "cleaned_data.csv"          # Output filename
      log_all_transformations: true                # Log all transformation steps (true/false)
      generate_comparison_report: true             # Generate before/after comparison (true/false)
      
    # === ADVANCED CLEANING OPTIONS (Available in CleaningConfig) ===
    # chunk_size: null                             # Chunk size for large datasets (1000-100000)
    # output_delimiter: ";"                        # CSV delimiter: ",", ";", "\t", "|"
    # output_quoting: 1                            # CSV quoting: 0=MINIMAL, 1=ALL, 2=NONNUMERIC, 3=NONE
    # output_encoding: "utf-8"                     # File encoding: "utf-8", "latin1", "cp1252"
    # remove_duplicates: true                      # Remove duplicate rows (true/false)
    # duplicate_subset: null                       # Columns for duplicate detection (null=all)
    # remove_low_variance: true                    # Remove low variance features (true/false)
    # low_variance_thresh: 1                       # Low variance threshold (0-10)
    # drop_high_miss_cols: true                    # Drop high missing columns (true/false)
    # missing_col_thresh: 0.3                      # Missing column threshold (0.1-0.8)
    # drop_high_miss_rows: false                   # Drop high missing rows (true/false)
    # missing_row_thresh: 0.5                      # Missing row threshold (0.2-0.9)
    # impute_num: "auto"                           # Numeric imputation: "auto", "mean", "median", "knn", "iterative"
    # impute_cat: "auto"                           # Categorical imputation: "auto", "most_frequent", "constant"
    # outlier_removal: true                        # Remove outliers (true/false)
    # outlier_method: "iqr"                        # Outlier method: "iqr", "isoforest", "zscore", "lof", "elliptic_envelope", "ensemble"
    # iqr_factor: 1.5                              # IQR factor for outlier detection (1.0-3.0)
    # iforest_contam: 0.01                         # Isolation Forest contamination rate (0.001-0.1)
    # zscore_thresh: 3.0                           # Z-score threshold (2.0-5.0)
    # lof_n_neighbors: 20                          # LOF number of neighbors (5-50)
    # lof_contamination: 0.01                      # LOF contamination rate (0.001-0.1)
    # elliptic_contamination: 0.01                 # Elliptic Envelope contamination rate (0.001-0.1)
    # elliptic_support_fraction: null              # Elliptic Envelope support fraction (null=auto, 0.1-1.0)
    # ensemble_methods: ['iqr', 'isoforest', 'zscore']  # Methods for ensemble outlier detection
    # ensemble_voting: "union"                     # Ensemble voting: "union", "intersection", "majority"
    # ensemble_contamination: 0.05                 # Overall contamination for ensemble (0.001-0.2)
    # skew_correction: true                        # Apply skew correction (true/false)
    # skew_thresh: 1.0                             # Skewness threshold (0.5-2.0)
    # skew_method: "yeo-johnson"                   # Skew method: "yeo-johnson", "box-cox", "log"

  # ML Advisory Settings
  ml_advisory:
    # === ASSUMPTION TESTING CONFIGURATION ===
    assumption_testing:
      enabled: true                                    # Enable assumption testing (true/false)
      
      # === NORMALITY TESTING ===
      normality_alpha: 0.05                          # Significance level for normality tests (0.01-0.1)
      normality_max_sample: 5000                     # Max samples for normality testing (1000-10000)
      normality_method: "kolmogorov"                    # Options: "shapiro", "kolmogorov", "anderson", "jarque_bera"
      
      # === HOMOSCEDASTICITY TESTING ===
      homo_alpha: 0.05                               # Significance level for variance tests (0.01-0.1)
      homo_method: "white"                   # Options: "breusch_pagan", "goldfeld_quandt", "white"
      
      # === MULTICOLLINEARITY TESTING ===
      vif_threshold: 9                            # VIF threshold for multicollinearity (5.0-50.0)
      correlation_threshold: 0.90                    # Correlation threshold (0.8-0.99)
      max_features_vif: 10                           # Max features for VIF calculation (10-100)
      
      # === CLASS BALANCE TESTING ===
      imbalance_threshold: 0.85                       # Threshold for class imbalance (0.6-0.95)
      min_class_size: 25                             # Minimum class size for analysis (10-100)
      
      # === LINEARITY & INDEPENDENCE TESTING ===
      linearity_alpha: 0.1                         # Significance level for linearity tests (0.01-0.1)
      independence_alpha: 0.1                       # Significance level for independence tests (0.01-0.1)
      
      # === PERFORMANCE & SCALABILITY ===
      chunk_size: 1000                               # Chunk size for large datasets (1000-50000, null=auto)
      enable_sampling: true                          # Enable sampling for large datasets (true/false)
      
      # === OUTPUT CONFIGURATION ===
      verbose: true                                  # Verbose assumption testing output (true/false)
      generate_recommendations: true                 # Generate model recommendations (true/false)
      
    # === MODEL RECOMMENDATION SETTINGS ===
    model_recommendation:
      enabled: true                                  # Enable model recommendations (true/false)
      include_ensemble_suggestions: true            # Include ensemble recommendations (true/false)
      consider_dataset_size: true                   # Consider dataset size in recommendations (true/false)
      performance_vs_interpretability: "balanced"   # Options: "performance", "interpretability", "balanced"
      
    # === META-LEARNING CONFIGURATION ===
    meta_learning:
      enabled: true                                  # Enable meta-learning features (true/false)
      performance_database: "metadata/model_performance.db"  # Path to performance database
      schema_similarity_threshold: 0.8              # Schema similarity threshold (0.5-0.95)
      min_historical_samples: 5                     # Min samples for meta-learning (3-20)
      transfer_learning: true                        # Enable transfer learning (true/false)
      
    # === FAIRNESS & BIAS DETECTION ===
    fairness:
      enabled: true                                  # Enable fairness analysis (true/false)
      protected_attributes: []                       # List of protected attributes (e.g., ["gender", "race"])
      fairness_metrics: ["demographic_parity", "equalized_odds", "calibration"]  # Fairness metrics to compute
      bias_threshold: 0.1                           # Bias detection threshold (0.05-0.2)
      mitigation_strategies: ["reweighting", "preprocessing", "postprocessing"]  # Available mitigation strategies

  # Model Training Settings
  model_training:
    # === DATA SPLITTING CONFIGURATION ===
    test_size: 0.2                      # Split ratio for test set (0.1-0.4)
    validation_size: 0.1                # Split ratio for validation set (0.0-0.3)
    random_state: 42                    # Random seed for reproducibility
    
    # === MODEL SELECTION ===
    max_models: 5                       # Maximum number of models to train (1-20)
    include_ensemble: true              # Include ensemble methods (true/false)
    include_advanced: true              # Include advanced algorithms (true/false)
    models_to_use:                      # Specific models to use (null for auto-select)
      - "RandomForestClassifier"        # Options: RandomForestClassifier, GradientBoostingClassifier
      - "GradientBoostingClassifier"    #         XGBClassifier, LogisticRegression, SVM, KNN
      - "XGBClassifier"                 #         AdaBoostClassifier, ExtraTreesClassifier
      - "LogisticRegression"
      - "KNN"                           #         LightGBMClassifier, CatBoostClassifier
      
    # === HYPERPARAMETER TUNING ===
    enable_tuning: true                 # Enable hyperparameter optimization (true/false)
    tuning_method: "optuna"               # Options: "grid", "random", "bayesian", "optuna", "none"
    tuning_iterations: 50               # Number of tuning iterations (10-500)
    tuning_cv_folds: 5                  # CV folds for tuning (3-10)
    
    # === OPTUNA HPO SETTINGS ===
    optuna_hpo_settings:
      sampler: "TPE"                    # Optuna sampler: "TPE", "Random", "CmaEs"
      pruner: "MedianPruner"           # Optuna pruner: "MedianPruner", "Hyperband", "SuccessiveHalving"
      direction: "maximize"             # Optimization direction: "maximize", "minimize"
      n_startup_trials: 10              # Number of startup trials (5-20)
      n_warmup_steps: 10               # Number of warmup steps (5-20)
      timeout_minutes: 30               # HPO timeout in minutes (15-120)
      study_name_suffix: "custom"       # Study name suffix
    
    # === VALIDATION CONFIGURATION ===
    cv_folds: 5                         # Cross-validation folds (3-10)
    scoring_strategy: "comprehensive"   # Options: "fast", "comprehensive", "custom"
    
    # === PERFORMANCE SETTINGS ===
    parallel_jobs: -1                   # Parallel jobs (-1=all cores, 1=single, 2-16=specific)
    max_training_time_minutes: 30       # Max training time limit (5-120 minutes, null=no limit)
    memory_limit_gb: null               # Memory limit in GB (1-32, null=no limit)
    
    # === PREPROCESSING OPTIONS ===
    encoding_strategy: "none"           # Options: "none", "auto", "onehot", "target", "binary"
    scaling_strategy: "none"            # Options: "none", "standard", "minmax", "robust", "quantile"
    
    # === BUSINESS FEATURES ===
    enable_business_features: false     # Enable business-specific features (true/false)
    feature_selection_enabled: false   # Enable automated feature selection (true/false)
    business_kpi_tracking: false       # Track business KPIs during training (true/false)
    business_rules_file: "config/business_rules.yaml"  # Path to business rules config
    business_kpis_file: "config/business_kpis.yaml"    # Path to business KPIs config
    
    # === OUTPUT CONFIGURATION ===
    save_models: true                  # Save trained models to disk (true/false)
    model_dir: "models"                 # Directory for saving models
    save_predictions: true              # Save model predictions (true/false)
    verbose: true                       # Verbose output during training (true/false)

  # Model Evaluation Settings
  model_evaluation:
    # === INPUT/OUTPUT CONFIGURATION ===
    models_dir: "models"                               # Directory containing trained models
    training_report_path: "comprehensive_training_report.json"  # Path to training report
    output_dir: "evaluation_results"                   # Output directory for evaluation results
    
    # === ANALYSIS COMPONENTS (Enable/Disable) ===
    enable_shap: false                                  # SHAP feature importance analysis (true/false)
    enable_learning_curves: true                      # Learning curve analysis (true/false)
    enable_residual_analysis: true                    # Residual analysis for regression (true/false)
    enable_stability_analysis: true                   # Model stability testing (true/false)
    enable_interpretability: true                     # Model interpretability analysis (true/false)
    enable_uncertainty_analysis: true                 # Uncertainty quantification (true/false)
    
    # === VISUALIZATION SETTINGS ===
    save_plots: true                                   # Save visualization plots (true/false)
    plot_format: "html"                               # Options: "html", "png", "svg", "both"
    plot_dpi: 300                                      # Plot resolution (150-600 DPI)
    figure_size: [12, 8]                              # Figure size [width, height] in inches
    
    # === PERFORMANCE SETTINGS ===
    n_permutations: 100                                 # Permutations for feature importance (10-500)
    n_bootstrap_samples: 200                          # Bootstrap samples for stability (50-1000)
    max_shap_samples: 1000                            # Max samples for SHAP analysis (100-5000)
    n_models_to_evaluate: 2                           # Number of top models to evaluate (1-10)
    
    # === FEATURE IMPORTANCE CONFIGURATION ===
    feature_importance:
      top_k_features: 12                              # Number of top features to display (5-50)
      show_direction: true                            # Show positive/negative impact (true/false)
      add_log_odds_axis: true                         # Add log-odds scale for classification (true/false)
      permutation_error_bars: true                    # Show error bars in permutation plots (true/false)
      
    # === STABILITY TESTING CONFIGURATION ===
    stability_testing:
      noise_levels: [0.01, 0.05, 0.1, 0.5]           # Noise levels for testing [0.001-0.5]
      dropout_rates: [0.05, 0.1, 0.2, 0.5]           # Feature dropout rates [0.01-0.5]
      random_state: 42                                # Random seed for reproducibility
      
    # === LOGGING CONFIGURATION ===
    verbose: true                                      # Verbose output (true/false)
    log_level: "INFO"                                 # Options: "DEBUG", "INFO", "WARNING", "ERROR"

# =============================================================================
# ðŸ”§ INFRASTRUCTURE & SYSTEM SETTINGS (shared by both modes)
# =============================================================================
infrastructure:
  # === FEATURE FLAGS ===
  feature_flags:
    data_quality_v2: true                             # Enable enhanced data quality system (true/false)
    mlflow_tracking: true                            # Enable MLflow experiment tracking (true/false)
    plugin_system: true                               # Enable plugin architecture (true/false)
    business_alignment: true                          # Enable business alignment features (true/false)
    metadata_tracking: true                           # Enable metadata tracking (true/false)
    lineage_tracking: true                            # Enable data lineage tracking (true/false)
    enhanced_logging: true                            # Enable enhanced logging features (true/false)
  
  # === MLFLOW CONFIGURATION ===
  mlflow:
    tracking_uri: "file:///mnt/c/Users/tony3/Desktop/tidytuesday/ds-autoadvisor/mlruns"        # MLflow tracking URI
    experiment_name: "ds-autoadvisor-v3"              # Experiment name for tracking
    artifact_location: "./mlruns"                     # Artifact storage location
    auto_log: true                                   # Enable automatic logging (true/false)
    log_system_metrics: true                        # Log system performance metrics (true/false)
    log_artifacts: true                              # Auto-log model artifacts (true/false)
  
  # === LOGGING CONFIGURATION ===
  logging:
    level: "INFO"                                     # Options: "DEBUG", "INFO", "WARNING", "ERROR"
    structured: true                                  # Use structured logging format (true/false)
    correlation_tracking: true                       # Track correlation IDs (true/false)
    performance_metrics: true                        # Log performance metrics (true/false)
    log_file: "logs/pipeline_v3.log"                 # Log file path
    console_output: true                              # Enable console output (true/false)
    
  # === PLUGIN SYSTEM ===
  plugins:
    enabled: true                                     # Enable plugin system (true/false)
    plugin_dir: "plugins"                            # Plugin directory path
    auto_discovery: true                              # Auto-discover plugins (true/false)
    validation_required: true                        # Require plugin validation (true/false)
    
    # === OPTUNA HPO PLUGIN ===
    optuna_hpo:
      enabled: true                                   # Enable Optuna HPO plugin (true/false)
      auto_yaml_generation: false                    # Auto-generate YAML configs (true/false) - DEPRECATED: Optuna handles best params automatically
      integrate_with_training: true                  # Integrate with training stage (true/false)
      integrate_with_evaluation: true                # Integrate with evaluation stage (true/false)
      fallback_to_default: true                      # Fallback to default config if HPO fails (true/false)
      output_recommendations: true                   # Output HPO recommendations (true/false)
      settings:
        default_iterations: 50                       # Default number of HPO iterations (20-200)
        default_cv_folds: 5                         # Default CV folds for HPO (3-10)
        default_sampler: "TPE"                      # Default sampler: "TPE", "Random", "CmaEs"
        default_pruner: "MedianPruner"              # Default pruner: "MedianPruner", "Hyperband", "SuccessiveHalving"
        storage_type: "sqlite"                       # Storage type: "sqlite", "memory"
        study_name_prefix: "ds_autoadvisor"         # Study name prefix
        timeout_minutes: 30                          # Timeout for HPO (15-120 minutes)
        parallel_jobs: 1                            # Parallel jobs for HPO (1-8)
        direction: "maximize"                        # Optimization direction: "maximize", "minimize"
        n_startup_trials: 10                        # Number of startup trials for TPE sampler (5-20)
        n_warmup_steps: 10                          # Number of warmup steps for pruning (5-20)
    
    # === EVIDENTLY MONITOR PLUGIN ===
    evidently_monitor:
      enabled: true                                   # Enable Evidently monitor plugin (true/false)
      train_vs_production: true                      # Enable train vs production comparison (true/false)
      drift_detection: true                          # Enable drift detection (true/false)
      performance_monitoring: true                   # Enable performance monitoring (true/false)
      html_reports: true                             # Generate HTML reports (true/false)
      slack_integration: true                        # Enable Slack integration (true/false)
      cron_support: true                             # Enable cron/scheduling support (true/false)
      synthetic_drift_demo: true                     # Enable synthetic drift demo (true/false)
      settings:
        drift_thresholds:
          data_drift: 0.5                           # Data drift threshold (0.1-0.9)
          column_drift: 0.5                         # Column drift threshold (0.1-0.9)
          missing_values: 0.1                       # Missing values threshold (0.05-0.3)
        alert_config:
          slack_webhook_url: null                    # Slack webhook URL (set by user)
          slack_channel: "#alerts"                   # Slack channel for alerts
          email_alerts: false                        # Enable email alerts (true/false)
          alert_levels: ["INFO", "WARNING", "CRITICAL"]  # Alert severity levels
        monitoring_schedule:
          enabled: false                             # Enable scheduled monitoring (true/false)
          frequency: "daily"                         # Frequency: "hourly", "daily", "weekly"
          time: "02:00"                             # Time to run (HH:MM format)
        report_retention:
          enabled: true                              # Enable report retention (true/false)
          retention_days: 30                        # Report retention period (7-365 days)
    
  # === METADATA STORAGE ===
  metadata:
    enabled: true                                     # Enable metadata tracking (true/false)
    store_type: "sqlite"                             # Options: "sqlite", "postgresql", "mysql"
    connection_string: "sqlite:///metadata/pipeline_metadata.db"  # Database connection string
    track_lineage: true                              # Track data lineage (true/false)
    track_performance: true                          # Track performance metrics (true/false)
    retention_days: 90                               # Metadata retention period (30-365 days)

# =============================================================================
# ðŸŽ¯ BUSINESS ALIGNMENT FEATURES (shared by both modes)
# =============================================================================
business_features:
  # === FEATURE SELECTION CONFIGURATION ===
  feature_selection:
    enabled: true                                     # Enable automated feature selection (true/false)
    methods:
      statistical: true                               # Use statistical feature selection (true/false)
      ml_based: true                                  # Use ML-based feature selection (true/false)
      business_rules: true                            # Use business rule-based selection (true/false)
    human_oversight:
      enabled: true                                   # Enable human oversight (true/false)
      approval_required: true                         # Require human approval (true/false)
      review_stages: ["initial_selection", "ml_validation", "final_confirmation"]  # Review stage names
    consistency_strategy: "intersection"             # Options: "intersection", "union", "voting"
    min_feature_count: 5                             # Minimum features to select (1-20)
    max_feature_count: 50                            # Maximum features to select (10-200)
    business_rules_file: "config/business_rules.yaml"  # Path to business rules config
    
  # === KPI TRACKING CONFIGURATION ===
  kpi_tracking:
    enabled: true                                     # Enable KPI tracking (true/false)
    config_file: "config/business_kpis.yaml"         # Path to KPI configuration file
    custom_kpis:
      revenue_impact:
        weight: 0.4                                   # Weight for revenue impact KPI (0.0-1.0)
        calculation: "churn_prevention_value - false_positive_cost"  # KPI calculation formula
      operational_efficiency:
        weight: 0.3                                   # Weight for operational efficiency KPI (0.0-1.0)
        calculation: "model_accuracy * process_automation_score"     # KPI calculation formula
      customer_satisfaction:
        weight: 0.3                                   # Weight for customer satisfaction KPI (0.0-1.0)
        calculation: "customer_satisfaction_score"    # KPI calculation formula
    roi_analysis:
      enabled: true                                   # Enable ROI analysis (true/false)
      time_horizon_months: 12                        # ROI analysis time horizon (6-36 months)
      discount_rate: 0.10                            # Discount rate for ROI calculation (0.05-0.20)
    correlation_tracking:
      enabled: true                                   # Enable correlation tracking (true/false)
      threshold_strong: 0.7                          # Strong correlation threshold (0.6-0.9)
      threshold_moderate: 0.4                        # Moderate correlation threshold (0.3-0.7)

# =============================================================================
# âš™ï¸ WORKFLOW & EXECUTION SETTINGS (shared by both modes)
# =============================================================================
workflow:
  # === PIPELINE STAGES ===
  stages:                                             # Pipeline execution order
    - "discovery"                                     # Data discovery and profiling
    - "cleaning"                                      # Data cleaning and preprocessing
    - "advisory"                                      # ML advisory and assumption testing
    - "training"                                      # Model training and selection
    - "evaluation"                                    # Model evaluation and analysis
  
  # === EXECUTION CONFIGURATION ===
  execution:
    parallel_execution: false                        # Enable parallel stage execution (true/false)
    stage_dependencies: true                         # Enforce stage dependencies (true/false)
    plugin_stages: ["feature_selection", "business_analysis"]  # Additional plugin stages
    skip_on_failure: false                           # Skip failed stages (true/false)
    retry_attempts: 3                                # Number of retry attempts (1-10)
    
  # === ERROR HANDLING ===
  error_handling:
    stop_on_error: false                             # Stop pipeline on first error (true/false)
    skip_failed_stages: true                         # Skip failed stages and continue (true/false)
    recovery_strategies: true                        # Enable error recovery strategies (true/false)
    generate_error_reports: true                     # Generate detailed error reports (true/false)
