# DS-AutoAdvisor Quick Start ğŸš€

**Get from CSV to trained models in 15 minutes with optional HPO and monitoring**

## ğŸ¯ The Complete Flow

```bash
# 1. Analyze your data (3-5 minutes)
python 01_data_discovery.py --data data/your_data.csv --target your_target

# 2. ğŸ›‘ STOP & EDIT: cleaning_config_template.yaml 
#    Location: pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml

# 3. Test your pipeline (2-10 minutes)
python 02_stage_testing.py --stage all

# 4. Package for production (1-3 minutes)
python 03_full_pipeline.py --enable-mlflow

# 5. View results (optional)
./mlflow_ui.sh
```

## ğŸ§© Plugin System - Enhanced ML Capabilities

**Two powerful plugins available for advanced ML workflows:**

### ğŸ¯ Optuna HPO Plugin
Automatically optimize hyperparameters with intelligent parameter space generation:

```bash
# Check plugin status
python plugin_manager.py status

# Generate HPO config from your data
python plugin_manager.py generate-hpo --data data/your_data.csv --target your_target --mode custom

# Use generated config in your pipeline by updating unified_config_v3.yaml
```

### ğŸ“Š Evidently Monitor Plugin  
Detect data drift and monitor model performance:

```bash
# Run monitoring demo with synthetic drift
python plugin_manager.py monitor-demo --reference-data data/your_data.csv --target your_target --drift-severity medium

# Set up scheduled monitoring (production)
python plugin_manager.py cron-setup --reference-data data/training_data.csv --current-data data/production_data.csv
```

**Key Features:**
- ğŸ”„ **Auto YAML Generation**: HPO configs generated from your data characteristics
- ğŸ“ˆ **Smart Model Selection**: Recommends optimal models for your dataset 
- ğŸš¨ **Drift Detection**: Compare training vs production data automatically
- ğŸ“± **Slack Integration**: Get alerts when drift is detected
- ğŸ­ **Synthetic Drift Demo**: Test monitoring with artificially generated drift
- âš™ï¸ **Seamless Integration**: Works with existing DS-AutoAdvisor pipeline

ğŸ“– **See [PLUGINS_README.md](PLUGINS_README.md) for complete plugin documentation**

---
## ğŸ“– Step-by-Step Guide

### Step 1: Data Discovery â±ï¸ 3-5 minutes

**What it does:** Analyzes your data and creates a cleaning template for you to customize.

```bash
# Basic usage (uses default data from config)
python 01_data_discovery.py

# With your own data
python 01_data_discovery.py --data data/bank.csv --target deposit

# For automation (skip human review)
python 01_data_discovery.py --auto-approve

# Enhanced analysis (more statistical details)  
python 01_data_discovery.py --enhanced-profiling
```

**You'll get:**
- ğŸ“Š `profiling_report.html` (visual data analysis)
- âš™ï¸ `cleaning_config_template.yaml` (THE FILE YOU NEED TO EDIT)
- ğŸ“ˆ Data quality scores and recommendations
- ğŸ’¾ Cached results for fast reuse

### Step 1.5: Edit Cleaning Template ğŸ›‘ REQUIRED

**Find the file:**
```bash
ls pipeline_outputs/01_discovery_*/configs/cleaning_config_template.yaml
```

**Open it and customize for each column:**

```yaml
column_transformations:
  age:
    imputation: 'median'        # median, mean, most_frequent, constant
    outlier_treatment: 'cap'    # cap, remove, winsorize, null
    scaling: 'standard'         # standard, minmax, robust
    
  salary:
    imputation: 'median'
    outlier_treatment: 'remove' # Remove extreme values
    transformation: 'log'       # Log transform for skewness  
    scaling: 'robust'           # Robust to outliers
    
  job_category:
    encoding: 'onehot'          # onehot, label, target, ordinal
    text_cleaning: true         # Clean text first
    max_categories: 20          # Limit high cardinality
```

**ğŸ’¡ Common Patterns:**
- **Financial data:** Use `log` transformation + `robust` scaling
- **Age/counts:** Use `median` imputation + `cap` outliers
- **Categories:** Use `onehot` (low cardinality) or `target` (high cardinality)
- **Text:** Enable `text_cleaning: true`

### Step 2: Test Your Pipeline â±ï¸ 2-10 minutes

**What it does:** Tests your cleaning rules and trains models using your specifications.

```bash
# Test everything (recommended first run)
python 02_stage_testing.py --stage all

# Test just cleaning (fast, good for iteration)
python 02_stage_testing.py --stage cleaning

# Test just training (when cleaning is finalized)
python 02_stage_testing.py --stage training

# Fast mode (fewer models, quicker results)
python 02_stage_testing.py --mode fast --stage training

# Use specific discovery results
python 02_stage_testing.py --stage all --discovery-dir pipeline_outputs/01_discovery_bank_20250811_140000
```

**Available stages:**
- **cleaning:** See your YAML rules in action
- **advisory:** Get model recommendations  
- **training:** Train multiple ML models
- **evaluation:** Compare model performance
- **all:** Run everything in sequence

**Key output:** `training_report.json` with model rankings and metrics

### Step 3: Package for Production â±ï¸ 1-3 minutes

**What it does:** Takes your best models and creates deployment-ready files.

```bash
# Auto-detect latest results (easiest)
python 03_full_pipeline.py

# With MLflow tracking (recommended)
python 03_full_pipeline.py --enable-mlflow

# Use specific Stage 2 results
python 03_full_pipeline.py --stage2-dir pipeline_outputs/02_stage_testing_custom_20250811_150000
```

**You'll get:**
- ğŸ“¦ `models/` - Your trained models ready to use
- ğŸ `model_server.py` - Python script to serve models
- ğŸŒ `api_template.py` - REST API template
- ğŸ³ `Dockerfile` - Container for deployment
- ğŸ“– Complete documentation and guides

**ğŸ¯ Important:** This step does NOT retrain. It packages existing models for speed.

### Step 4: View Results (Optional)

```bash
# Start MLflow UI
./mlflow_ui.sh

# Or specific port
./mlflow_ui.sh 5001

# Visit http://localhost:5000 in browser
```

---
## ğŸ”„ Common Workflows

### ğŸ†• First Time With New Dataset
```bash
python 01_data_discovery.py --data data/new_data.csv --target outcome
# Edit cleaning_config_template.yaml
python 02_stage_testing.py --stage all  
python 03_full_pipeline.py --enable-mlflow
```

### âš¡ Quick Experiment
```bash
# If you have discovery results already
python 02_stage_testing.py --mode fast --stage training
python 03_full_pipeline.py
```

### ğŸ”§ Iterative Cleaning Development
```bash
# Test cleaning changes quickly
python 02_stage_testing.py --stage cleaning
# Edit cleaning_config_template.yaml based on results
python 02_stage_testing.py --stage cleaning  # Test again
python 02_stage_testing.py --stage training  # When satisfied
```

### ğŸ­ Production Run
```bash
python 01_data_discovery.py --auto-approve
python 02_stage_testing.py --stage all
python 03_full_pipeline.py --enable-mlflow
```

---
## ğŸ“ Key Files You Need to Know

| File | Location | What It Does |
|------|----------|--------------|
| **cleaning_config_template.yaml** | `01_discovery_*/configs/` | **YOU EDIT THIS** - Controls all data cleaning |
| **training_report.json** | `02_stage_testing_*/` | Shows model rankings and performance |
| **models_index.json** | `03_production_*/models/` | Lists your packaged models |
| **model_server.py** | `03_production_*/deployment/` | Ready-to-run model serving script |

---
## âš ï¸ Important Notes

### ğŸ›‘ Don't Skip the Manual Step
The cleaning template edit is REQUIRED. The pipeline generates smart defaults, but you need to review and customize for your specific data and use case.

### ğŸ”„ When to Re-run Steps
- **New data?** â†’ Re-run Step 1
- **Better cleaning ideas?** â†’ Edit template â†’ Re-run Step 2  
- **Need different models?** â†’ Adjust config â†’ Re-run Step 2
- **Ready to deploy?** â†’ Re-run Step 3

### âš¡ Speed vs Quality Tradeoffs
- **Stage 2 fast mode:** Fewer models, quicker results
- **Stage 2 custom mode:** More models, better performance  
- **Step 3 no retrain:** Fast packaging, but metrics from Stage 2 snapshot

---
## ğŸš¨ Troubleshooting

| Problem | Solution |
|---------|----------|
| "No discovery directory found" | Run Step 1 first |
| "cleaning_config_template.yaml not found" | Check Step 1 completed successfully |
| "No models trained" | Check target column name, verify data quality |
| "MLflow UI empty" | Use `--enable-mlflow` flag in Step 3 |
| Models won't serve | Check `models/` directory has .pkl files |

---
## ğŸ¯ Pro Tips

### ğŸš€ For Speed
```bash
# Skip reviews, use fast mode
python 01_data_discovery.py --auto-approve
python 02_stage_testing.py --mode fast --stage training
python 03_full_pipeline.py
```

### ğŸ” For Quality  
```bash
# Enhanced profiling, full model search
python 01_data_discovery.py --enhanced-profiling
# Carefully edit cleaning_config_template.yaml
python 02_stage_testing.py --mode custom --stage all
python 03_full_pipeline.py --enable-mlflow
```

### ğŸ§ª For Development
```bash
# Test components separately
python 02_stage_testing.py --stage cleaning
python 02_stage_testing.py --stage training
```

---
## ğŸ‰ Success Indicators

You know it's working when:
- âœ… Step 1 generates `cleaning_config_template.yaml`
- âœ… Step 2 creates `training_report.json` with model rankings  
- âœ… Step 3 creates `models/` directory with .pkl files
- âœ… MLflow UI shows your experiments (if enabled)
- âœ… `model_server.py` loads and runs without errors

**Ready to start?** Pick your workflow above and go! ğŸš€
